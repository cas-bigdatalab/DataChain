#kafka
zookeeper.connect=10.0.71.20:2181,10.0.71.26:2181,10.0.71.27:2181
kafka.brokerList=10.0.71.20:9092,10.0.71.26:9092,10.0.71.27:9092
group.id=test-consumer-group

#spark streaming seconds
duration=1

#mysql
mysql_url=jdbc:mysql://10.0.71.7:3306/
mysql_user=root
mysql_password=root
mysql_driver=org.apache.spark.sql.jdbc

#hbase
hbase_driver=org.apache.hadoop.hive.hbase.HBaseStorageHandler

#mongodb
mongo_host=10.0.71.26:27017
mongo_driver=com.stratio.datasource.mongodb

#hivedb
hive_db=hive,hbase,impala

#solr
solr_driver=solr
solr_zkhost=10.0.71.14:2181,10.0.71.17:2181,10.0.71.38:2181

#memcache
memcache_address=10.0.20.47:11211
memcache_driver=cn.cnic.bigdatalab.spark.memcached

mysql_create_sql=CREATE TEMPORARY TABLE %tablename% USING %using% OPTIONS ( url '%url%', dbtable '%table%')
mongo_create_sql=CREATE TEMPORARY TABLE %tablename% ( %columns% ) USING %using% OPTIONS ( host '%host%', database '%db%', collection '%table%')
hive_create_sql=CREATE TABLE IF NOT EXISTS %tablename%( %columns% ) partitioned by( mp int, %partitions%)
hbase_create_sql=CREATE TABLE IF NOT EXISTS  %tablename% ( %columns% ) STORED BY '%using%' WITH SERDEPROPERTIES ('hbase.columns.mapping' = '%hbase_columns%' ) TBLPROPERTIES ('hbase.table.name' = '%hbase_tablename%', 'hbase.mapred.output.outputtable' = '%hbase_tablename%')
solr_create_sql=CREATE TEMPORARY TABLE %tablename% USING %using% OPTIONS ( zkhost  '%zkhost%', collection '%table%', soft_commit_secs '1', gen_uniq_key 'true', fields '%columns%')
memcache_create_sql=CREATE TEMPORARY TABLE  %tablename% ( %columns% ) USING %using% OPTIONS (address '%address%', key '%table%')
create_sql_separator=#-#

#spark param
spark_host=10.0.71.1
spark_host_user=root
spark_host_password=cnic.cn
master=spark://10.0.71.1:7077
#master=yarn-client
executor-memory=10G
total-executor-cores=12
spark_home=/opt/spark-1.6.1-bin-hadoop2.6/

#task param
realtime_sql_class=cn.cnic.bigdatalab.compute.realtime.SQLCompute
realtime_external_class=cn.cnic.bigdatalab.compute.realtime.ExternalCompute
store_sql_class=cn.cnic.bigdatalab.compute.realtime.SQLCompute
offline_sql_class=cn.cnic.bigdatalab.compute.offline.Offline
datachain_path=/opt/datachain.jar

#flume param
flume_home=/opt/apache-flume-1.6.0-bin
flume_conf_localDir=/tmp
flume_log_dir=/opt/flume.out

#ssh dsa path
ssh_dsa_path=/Users/xjzhu/.ssh/id_dsa

#json file path
json_path=D:/git/DataChain/configure/

#SDK method
sdk_method=process

#external path
external_path=/opt/external/

#server
server_ip=10.0.71.32
server_port=9999

#hive merge sql
hive_merge=INSERT INTO TABLE %tablename% PARTITION (mp=24, %partition_columns%) SELECT %columns%, %partition_columns% FROM %tablename% WHERE mp=preMP
hive_truncate=TRUNCATE TABLE %tablename% PARTITION (mp=preMP)




