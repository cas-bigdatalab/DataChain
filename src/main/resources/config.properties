#kafka
zookeeper.connect=10.0.50.216:2181
group.id=test-consumer-group

#spark streaming seconds
duration=1

#mysql
mysql_url=jdbc:mysql://10.0.50.216:3306/
mysql_user=root
mysql_password=root
mysql_driver=org.apache.spark.sql.jdbc


#mongodb
mongo_host=172.16.106.3:27017
mongo_driver=com.stratio.datasource.mongodb

#hivedb
hive_db=hive,hbase,impala

mysql_create_sql=CREATE TEMPORARY TABLE %tablename% USING %using% OPTIONS ( url '%url%', dbtable '%table%')
mongo_create_sql=CREATE TEMPORARY TABLE test( %columns% ) USING %using% OPTIONS ( host '%host%', database '%db%', collection '%table%')
hive_create_sql=CREATE TABLE IF NOT EXISTS %tablename%( %columns% )
#spark param
spark_host=10.0.50.216
spark_host_user=root
spark_host_password=bigdata
master=spark://10.0.50.216:7077
executor-memory=10G
total-executor-cores=12
spark_home=/opt/spark-1.6.1-bin-hadoop2.6/

#task param
realtime_class=cn.cnic.bigdatalab.compute.RealTime.Kafka2SparkStreaming
realtime_path=/opt/datachain.jar
offline_class=cn.cnic.bigdatalab.compute.Offline
offline_path=/Project/DataChain/classes/artifacts/datachain_jar/datachain.jar

#flume param
flume_home=/usr/lib/flume
flume_conf_localDir=/tmp

#ssh dsa path
ssh_dsa_path=/Users/xjzhu/.ssh/id_dsa

